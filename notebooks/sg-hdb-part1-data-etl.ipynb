{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://gallery.mailchimp.com/f98d5ac0a3fbbdcdda35136ab/images/2002af76-5fd4-4185-9d49-28558b6b8772.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sg-hdb-resale-bokeh` \n",
    "# Part 1: Extract, Transform, Load\n",
    "\n",
    "In this notebook, I will be carry out steps and constructing structures that allows the following:\n",
    "+ preliminary exploration of the raw data in terms of its state and format that it comes in.\n",
    "+ storing of extracted data in class variables\n",
    "+ loading extracted data into a database\n",
    "\n",
    "These things takes one through the **ETL** process that is facilitated by a set of functions/classes that makes a data pipeline. Albeit a simple one, beginners can learn from this whole process that we're about to go through.\n",
    "\n",
    "<img src=\"https://i.ibb.co/wJQ4fK7/etl-workflow-image.png\">\n",
    "\n",
    "What is **ETL**?\n",
    "+ **Extract:** This is the process of extracting data/information from the raw files. In our context here, the raw files have been provided to us in CSV form. In other enterprise use cases these raw files can come in other forms such as streamed JSON objects or transactional data from OLTP databases.\n",
    "+ **Transform:** The process of converting data from the aforementioned extraction process to a digestible format to be ingested to another database or a datalake.\n",
    "+ **Load:** Following transformation where the extracted data has been reformated, the process of loading all of it into a database comes under here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by importing the packages we need\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><h1><center>Extract</center></h1></font>\n",
    "Download data at:\n",
    "+ https://data.gov.sg/dataset/resale-flat-prices\n",
    "+ https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "\n",
    "__Resale Flat Prices:__ This dataset consist of transactions for HDB resale units.\n",
    "\n",
    "This section is where we construct some functions to carry out the main objective of extracting the data from the CSV files containing information regarding HDB resale units. A list of the things that we'd like to have the functions carry out:\n",
    "+ Checking the names (and their naming conventions) of the CSV files\n",
    "+ Checking the number of header columns that exist within each file and see if they tally with each other\n",
    "+ Checking the common names of headers that exist across all the files\n",
    "+ Combine/concatenate all the data into one single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing down the list of files in the relevant directory\n",
    "# Where prefixed with `!`, a shell runs the command\n",
    "!ls ../data/raw/resale-flat-prices/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of several things to do:\n",
    "+ List down files ending with .csv\n",
    "+ Check length of columns if they are the same\n",
    "+ Check if the names of the columns are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, one .csv file, particularly the first one, returns 11 columns while the rest with 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From mere observation, we can see that the column `remaining_lease` is the odd one out. Below, we can identify the common column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it is seen that we have 10 common column names across all the datasets.\n",
    "\n",
    "The function that we will be creating below is to combine all the data from all the .csv files into one `pandas` dataframe. We will be using the `pandas` function `concat` to combine. Where a dataset does not have a certain variable that exists in the other dataset, the variable will be retained while filling in '0's for empty values. This is specifically referring to the variable `remaining_lease`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import the data containing the quarterly HDB resale price index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all is good with the imported dataset for HDB resale price indexes. We will not bother with it for the following transformation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><h1><center>Transform</center></h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we have extracted from the CSV files are quite clean and hence we can choose to not do any transformation prior to the loading process. Of course, in the real world, hardly ever do we get such luck.\n",
    "Further transformations for the purpose of feature engineering can be implemented during the [modelling phase](./sg-hdb-part2-modelling.ipynb).\n",
    "\n",
    "Even though the formatting/state of the dataset is good enough for us to ingest into a database, for the purpose of this exercise, let us transform the values of a single variable.\n",
    "\n",
    "Currently, as seen below, the variable `flat_model` contains many (35) different categories and some are mismatched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many different categories but some of them are linked to the same one category and are actually just spelled differently due to entry methods. For example we have the following categories as observed from above:\n",
    "+ 'Model A'\n",
    "+ 'MODEL A'\n",
    "\n",
    "Both are pertaining to a single model category but due to the different casings they are treated as different categories. A simple act of transformation that we can employ through a class that we will be creating is to just convert every letter of the values in the `flat_model` column to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, through transformation we are able to handle mismatched categories and in this sense, we have only done some form of preliminary data cleaning but that of course does not deviate from the essence of the transformation process.\n",
    "\n",
    "Let us now export the extracted data to one single .csv file for checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><h1><center>Load</center></h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following extraction and transformation, we now intend to load the data derived from the above processes into a simple [SQLite](https://www.sqlite.org/index.html) RDMS/database. \n",
    "(For simplicity's sake, we'll use SQLite for now. In the future, one might want to take a look into remote alternatives.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check max length of a value in a column of object data type\n",
    "hdb_combi_df.town.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of null values across all columns\n",
    "hdb_combi_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sqlalchemy\n",
    "from sqlalchemy import Table, Column, Integer, String, Float, Date\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick observation on how the datasets that we intend to load into the database looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdb_combi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdb_rpi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next few cells, we will be working towards creating the database:\n",
    "+ create engine to initialise connection\n",
    "+ specify table names and their columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine\n",
    "engine = create_engine('sqlite:///../data/processed/sg_hdb.db')\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify tables\n",
    "class HDBRes(Base):\n",
    "    __tablename__ = 'sg_hdb_resale'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    block = Column(String(7))\n",
    "    flat_model = Column(String(30))\n",
    "    flat_type = Column(String(20))\n",
    "    floor_area_sqm = Column(Float())\n",
    "    lease_commence_date = Column(Integer())\n",
    "    month = Column(String(7))\n",
    "    remaining_lease = Column(Integer())\n",
    "    resale_price = Column(Float())\n",
    "    storey_range = Column(String(15))\n",
    "    street_name = Column(String(50))\n",
    "    town = Column(String(20))\n",
    "    \n",
    "class HDBPI(Base):\n",
    "    __tablename__ = 'sg_hdb_pi'\n",
    "    \n",
    "    quarter = Column(String(7), primary_key=True)\n",
    "    index = Column(Float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables as defined above\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a function that allows us to connect with the database created from above and insert values from relevant `pandas` dataframes into the SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGHDBBulkInsert(table_name, df_to_insert, engine_loc):\n",
    "    engine = create_engine(engine_loc)\n",
    "    \n",
    "    # The orient='records' is the key of this, it allows to align with the format mentioned in the doc to insert in bulks.\n",
    "    list_to_write = df_to_insert.to_dict(orient='records')\n",
    "    metadata = sqlalchemy.schema.MetaData(bind=engine)\n",
    "    table = sqlalchemy.Table(table_name, metadata, autoload=True)\n",
    "    \n",
    "    # Open the session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    conn = engine.connect()\n",
    "    # Insert the dataframe into the database in one bulk\n",
    "    conn.execute(table.insert(), list_to_write)\n",
    "    # Commit the changes\n",
    "    session.commit()\n",
    "    # Close the session\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing insertion of the HDB Resale data\n",
    "SGHDBBulkInsert('sg_hdb_resale', hdb_combi_df, 'sqlite:///../data/processed/sg_hdb.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the HDB Resale Price Indexes\n",
    "SGHDBBulkInsert('sg_hdb_pi', hdb_rpi, 'sqlite:///../data/processed/sg_hdb.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe as to whether the intended operations have been executed successfully, we can use relevant GUI tools to examine the contents of databases. For SQLite, we can use [DB Browser for SQLite](https://sqlitebrowser.org/dl/). Once we have loaded the relevant data into our database, it is time for us to work on a simple machine learning model. On to the next part [here](./sg-hdb-part2-modelling.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
